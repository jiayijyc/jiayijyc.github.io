---
layout: post
title:  "Data Science"
---

# **II Model Evaluation**

## Introduction
The world has witnessed an increasingly important role that machine learning (ML) plays in both academic and business areas to analyze data, make predictions, make data-driven recommendations and decisions. While designing an algorithm and training a model is a key step, the model evaluation plays an essential role as well and cannot be neglected in machine learning pipeline. The problems we need to figure out in model evaluation include but not limited to 

* Is the model accurate enough and can we trust the predictions the model makes?
* Could the model be just memorizing the training data, and consequently not able to make good predictions on unseen data?
* Which model should I choose among the pool of candidates? </br>

In this article, we will introduce a common technique used to assess how well a model genralizes to out-of-sample data. Additionally, we will explain the model evaluation metrics for both supervised and unsupervised learning as well. 

## Model Evaluation Technique
### Cross Validation
## Model Evaluation Metrics

## 1. **Supervisned Learning**
### 1.1 Regression
#### **Mean Absolute Error (MAE)**
$$MAE = \frac{1}{n}\sum_{j=1}^n|y_{j}-\hat{y_{j}}|$$

#### **Root Mean Squared Error (RMSE)**
$$RMSE = \sqrt{frac{1}{n}\sum_{j=1}^n (y)^2}$$

#### **Coeficient of Determination**

#### **Standardized Residuals Plot**



### 1.2 Classification



## 2. **Unsupervised Learning**




